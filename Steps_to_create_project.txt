-------------- Industry Ready Machine Learning Project --------------

			
		   ___Part - 1___
Agenda:
1. Creation Github repository.
	- Adding readme.md and .gitignore file.
2. Creation of requirements.txt
3. Writing setup.py file.

Steps:
1. Github Repositry Creation.
	- cmd -> "code ." to open vs code from particular folder.
2. Create and activate a virtual envirnoment.
	- cmd -> "conda create -p venv python==3.8 -y" to create virtual environment.
	- make sure that you have conda -> "conda --version" if conda is not there then install.
	- cmd -> "conda activate venv/" to activate the created virtual environment.
3. Clone the github repository in local machine.
	- link you github account to local machine
		- "git config --global user.name '<user name>'"
		- "git config --global user.email <email>"
	1. cmd -> "git init"
	2. cmd -> "git remote add origin <git repo link>"
	3. Create "README.md" to write about projec.
	4. cmd -> "git add <file name>" to track the file.
	5. cmd -> "git commit -m '<commit msg>'" to add file to staging area.
	6. cmd -> "git push -u origin master" to push code to github.
	7. Create ".gitignore" to ignore unwanted files and folders in github.
	8. cmd -> "git pull -u origin master" to pull change made directly in github.
4. Create "requirements.txt" and add all the dependencies.
	- add "-e ." used to trigger "setup.py"
5. Create "setup.py" which will create our project as package.
	- create a function which will return list which contain all the package name present in requirements.txt but we have to remove "-e ." as it is used to trigger setup.py file.
	- to check this: cmd -> "pip install requirements.txt"
6. Create "src" folder which contain our PROJECT CODE and inside that create "__init__.py" which will be contain code.


		   ___Part - 2__
Agenda:
1. Create project folder structure.
2. Add exception handling code.
3. Add logging code.

Steps:
1. Now, Create the project folder structure by creating "src" folder.
2. Create a "components" folder inside "src" folder.
	- __init__.py
	- data_ingestion.py
	- data_transformatino.py
	- model_trainer.py
3. Create a "pipeline" folder inside "src" and add files.
	- __init__.py
	- train_pipeline.py
	- predict_pipeline.py
4. Add the following files into "src" folder.
	- __init__.py
	- exception.py
	- logger.py
	- utils.py
5. Add exception handling inside "exception.py" using sys module and write custom exception module.
6. Add logging inside "logger.py" using logging module.
- for big project even more file will be added inside the "components" folder such as data validation, model evaluation.


		   ___Part - 3__
Agenda:
1. Understanding problem Statement.
2. Performing EDA in jupyter notebook.
3. Model trianing in jupyter notebook.
	- Doing feature engineering using column_transform

Steps:
1. Create "notebook" folder then add the following files.
	- Create "data" folder contains dataset.
	- Perform eda.
	- Perform model training.


		   ___Part - 4__
Agenda:
1. Writing module for data ingestion which read data and do train test split.

Steps:
1. Write code in "data ingestion.py" to read data.
	- split that dataframe into "train" and "test" set and save in "artifacts" folder.


		   ___Part - 5__
Agenda:
1. Writing module for data transformation which takes train data and test data then split them independent and dependent features 

Steps:
1. Write code in "data_transformation.py" to do data transformation.
	- Here, we will handle missing values, duplicate values and we will do standard scaling and data encoding.
	- using column transformer to combine the numerical and categorical pipeline.


		   ___Part - 6__
Agenda:
1. Writing module for model training where model is trained with differenct algorithm then select best accuracy model then save model.

Steps:
1. Write Code in "model_trainer.py" to do model training and evaluation.
	- add "evaluate_models" function in "utils.py"


		   ___Part - 7__
Agenda:
1. Add hyper tuning parameters inside "model_trainer.py" and done hyperparameter tuning inside evalutate model function present inside "utils.py"

Steps:
1. Add hypertuning parameters in "model_trianer.py" as dictionary and send to evaluate_model function present in "utils.py".
2. Perform hyper parameter tuning inside utils.py module using parameters.


		   ___Part - 8__
Agenda:
1. Create Flask web application "app.py".
	- Creating required html pages.
	- Creating required css pages.
2. Create prediction pipeline "predict_prediction.py" 
	- which customizes the data and also returns data as dataframe.
	- which take data given by the user and returns the prediction.

Steps:
1. Create "predict_pipeline.py" and add 2 class.
	- 1st class returns returns data as dataframe.
	- 2nd class take input data as dataframe and returns the prediction.
2. Create "templates/home.html" file and create a basic form that takes input.
3. Create "static/css/style.css" file and add styling to the html page.
4. Create a flask web app that take data from user and returns the prediction results.
5. Add the route which we can use to test api.
6. To check flask application.
	- run 'python application.py' in virtual envirnoment terminal.


		   ___Part - 9__
Agenda:
1. Deployment of project on aws using elastic beanstalk.

Steps:
1. Create ".ebextensions" folder and add "python.config" file.
2. Create IAM role.
3. Create elasticbean stack service.
4. Create Codepipeline which connect code present in github and elasticbean stack service.


		   ___Part - 10__
Agenda:
1. Deployment of project on Azure using "web app" service


		   ___Part - 11__
NOTE: folder '.ebextensions' is not required while deploying using aws ec2 and ecr. 

Agenda:
1. Deployment of project as a docker images in aws ec2 instance by creating docker image inside aws ecr.
2. Automating integration and deployment using using github actions by adding workflow to project.

Steps:
1. Write "Dockerfile" and build the application.
	- cmd -> "docker build -t student_perfomance ." to build docker image and check image is working properly in local machine.
2. Create github workflow in "main.yml" file
	- Implement CI-CD pipeline using github actions.
3. Create a iam user.
	- add "AmazonEC2ContainerRegistryFullAccess" and "AmazonEC2FullAccess" policies.
4. Create a repositry ECR.
5. Create a EC2 instance of required docker size.
	- in network settings tick the check box all https and http.
6. Connect to EC2 instance and install required dependencies in terminal.
	- connect instance through "ec2 instance connect".
	- optional:
	- cmd -> "sudo apt-get update -y"
	- cmd -> "sudo apt-get upgrade"
	- Required: to install necessary package that is required for docker.
	- cmd -> "curl -fsSL https://get.docker.com -o get-docker.sh" 
	- cmd -> "sudo sh get-docker.sh" 
	- cmd -> "sudo usermod -aG docker ubuntu"
	- cmd -> "newgrp docker"
	- to check docker is running cmd -> "docker --version"
7. Set up app runner.
	- go to github > settings in that page go to Actions > Runner.
	- Create new self-hosted runner.
	- select linux machine.
	- copy the commands given and run them in ec2 instance terminal. 
		- Below are the steps present there.
		- Download
		- Configure
			- for name of runner group just click enter(means default).
			- name of runner : "self-hosted"
			- for additional labels also click enter.
			- name of work folder also click enter.
		- Using your self-hosted runner.
	- go to github > settings in that page go to Actions > Runner we can see "self-hosted" app runner in idle status.
	- go to github > settings in that page go to "Secrets and variables" > actions and add "5 keys".
		1. AWS_ACCESS_KEY_ID
		2. AWS_SECRET_ACCESS_KEY
		3. AWS_REGION 
		4. AWS_ECR_LOGIN_URI - paste the ecr uri but remove ecr repo name present at the end of link.
		5. ECR_REPOSITORY_NAME - go to aws ecr and check the ecr repository created and give that name.
8. Run Workflow.
	- do some changes and commit it.
	- go to "actions" tab we can see there a ci cd pipeline automatically starts.
	- to open our web app
		- go to aws ec2 instance and select our ec2 instance.
		- click on "public ipv4 address".
		- intially we will get error so we have to add security group.
		- in same instance go to "security" and click on "security groups".
		- now click on "inbound rules" and add "custom tcp" and port range as "8080" which is given in application.py and source is custom.
		- then go back and refresh and copy ipv4 address.
		- open new tab and paste ipv4 address and give port number.(ex: "http://3.83.159.89:8080")
9. after finishing go to github remove "app runner" and also terminate ec2 instance and delete ecr repo and iam user.








